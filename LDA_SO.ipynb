{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore, CoherenceModel, Phrases, LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original data\n",
    "data_path = \"\"\n",
    "\n",
    "\n",
    "#LDA PARAMETERS\n",
    "num_topics_range = range(5,60,5)\n",
    "alpha = 0.01\n",
    "beta = 0.1\n",
    "passes = 500\n",
    "random_state=69\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assign_topics_to_documents(lda_model, corpus, data):\n",
    "    document_topics = []\n",
    "    for doc_bow in corpus:\n",
    "        doc_topics = lda_model.get_document_topics(doc_bow)\n",
    "        doc_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
    "        most_probable_topic = doc_topics[0][0] if doc_topics else None\n",
    "        document_topics.append(most_probable_topic)\n",
    "    data['Assigned_Topic'] = document_topics\n",
    "\n",
    "    # Count the number of posts per topic\n",
    "    topic_counts = data['Assigned_Topic'].value_counts()\n",
    "\n",
    "    # Sort data by 'Assigned_Topic'\n",
    "    sorted_data = data.sort_values(by='Assigned_Topic')\n",
    "    return sorted_data, topic_counts\n",
    "\n",
    "\n",
    "def save_topics(lda_model, num_topics, directory, topic_counts):\n",
    "    topics = lda_model.print_topics(num_topics=num_topics, num_words=15)\n",
    "    with open(os.path.join(directory, f'{num_topics}_topics.csv'), 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Topic ID', 'Topics', 'Post Count'])  # Column headings change to fit data\n",
    "        for topic_id, topic in enumerate(topics):\n",
    "            count = topic_counts.get(topic_id, 0)  # Get count for each topic, default to 0\n",
    "            writer.writerow([topic_id, topic, count])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "texts = data[''].apply(lambda x: x.split()) #input col name \n",
    "\n",
    "bigram = Phrases(texts, min_count=5, threshold=100)\n",
    "texts_bigram = [bigram[doc] for doc in texts]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts_bigram)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_bigram]\n",
    "\n",
    "coherence_scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def topic_cosine_similarity(lda_model):\n",
    "    topic_word_matrix = lda_model.get_topics()  # Shape: num_topics x vocabulary_size\n",
    "    sim_matrix = cosine_similarity(topic_word_matrix)\n",
    "    np.fill_diagonal(sim_matrix, 0)\n",
    "    avg_similarity = np.sum(sim_matrix) / (lda_model.num_topics * (lda_model.num_topics - 1))\n",
    "    return avg_similarity\n",
    "\n",
    "perplexity_values = []\n",
    "average_similarities = []\n",
    "\n",
    "for num_topics in num_topics_range:\n",
    "    print(\"Running for topic: \", num_topics)\n",
    "    lda_model = LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, alpha=alpha, eta=beta, passes=passes, workers=min(8, 9), random_state=random_state)\n",
    "\n",
    "    perplexity_values.append(lda_model.log_perplexity(corpus))\n",
    "    average_similarities.append(topic_cosine_similarity(lda_model))\n",
    "\n",
    "    directory = f\"{num_topics}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    updated_data, topic_counts = assign_topics_to_documents(lda_model, corpus, data)\n",
    "    save_topics(lda_model, num_topics, directory, topic_counts)\n",
    "    updated_data.to_csv(os.path.join(directory, f'{num_topics}_annotated_data.csv'), index=False)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(num_topics_range, perplexity_values, marker='o')\n",
    "plt.title(\"Perplexity vs Number of Topics\")\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_topics_range, average_similarities, marker='o', color='r')\n",
    "plt.title(\"Average Cosine Similarity vs Number of Topics\")\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Average Cosine Similarity\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of visulizing data\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "#lda_model = LdaMulticore(corpus,num_topics=25,id2word=dictionary,alpha=alpha,eta=beta,passes=passes,workers = min(8,9),random_state=random_state)\n",
    "#vis_data = gensimvis.prepare(lda_model,corpus,dictionary)\n",
    "#pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
